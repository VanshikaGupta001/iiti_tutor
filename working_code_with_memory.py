
# -*- coding: utf-8 -*-
"""DB copy of Integration_IITIMentor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZSdRYwTnNSN-HiFmPf2gScfe1qQAISMb
"""

!pip install motor

!pip install faiss-cpu #While using CPu

!pip install faiss-cpu #While using CPu
!pip install pdf2image
!apt-get install -y poppler-utils
!apt-get install -y tesseract-ocr
!pip install pytesseract
!pip install pillow
!pip install openai
!pip install reportlab
!pip install pymongo
!pip install pyngrok


import torch
import gc
gc.collect()
torch.cuda.empty_cache()

import os
import json
import re
from openai import OpenAI
import datetime

from typing import Optional
# Google Calendar API integration
import asyncio
from concurrent.futures import ThreadPoolExecutor

import httpx
import io

import shutil
import tempfile
from datetime import datetime, timezone  # Add timezone to imports
from fastapi import FastAPI, File, UploadFile, Form , Response , Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from reportlab.lib.pagesizes import A4
import uvicorn
import uuid
import motor.motor_asyncio
import psutil, os
def print_memory_usage(tag=""):
    print(f"[MEMORY {tag}] Used: {psutil.Process(os.getpid()).memory_info().rss / 1024**2:.2f} MB")

class ChatHistoryManager:
    def __init__(self, mongo_client):
        self.client = mongo_client
        self.db = self.client["IITI_Tutor_DB"]
        self.collection = self.db["chat_history"]

    async def save_message(self, user_id: str, conversation_id: str, role: str, message_content: str):
        print("Starting to save message...")
        message = {
            "user_id": user_id,
            "conversation_id": conversation_id,
            "role": role,
            "content": message_content,
            "timestamp": datetime.now(timezone.utc)
        }
        print("Message object created:", message)
        try:
            await self.collection.insert_one(message)
            print("Message successfully saved to database")
        except Exception as e:
            print(f"Database save error: {str(e)}")
            raise

    async def load_history(
    self,
    user_id: str,
    conversation_id: str,
    limit: int = 5
) -> list[dict]:
        """Fetch chat history in chronological order (oldest first)."""
        try:
            # Validate IDs (optional but recommended)
            if not isinstance(user_id, str) or not isinstance(conversation_id, str):
                raise ValueError("Invalid user_id or conversation_id")

            # Query: Fetch only needed fields, newest first
            cursor = self.collection.find(
                {"user_id": user_id, "conversation_id": conversation_id},
                projection={"role": 1, "content": 1, "_id": 0}  # Exclude _id, include only role/content
            ).sort("timestamp", -1).limit(limit)

            # Convert to list and reverse to chronological order
            history = [doc async for doc in cursor]
            return history[::-1]  # Oldest first

        except Exception as e:
            # Log the error (e.g., using logging.getLogger(__name__))
            print(f"Error loading history: {e}")
            return []  # Fail gracefully

class QueryBot:
    def __init__(self,
                 embedding_model="all-MiniLM-L6-v2",
                 groq_api_key="gsk_fi9VBbpzKaLwCD4rt4q",
                 model_name="llama-3.1-8b-instant",
                 groq_api_url="https://api.groq.com/openai/v1/chat/completions"):

        # self.JSON_FILE = json_file
        self.EMBEDDING_MODEL = embedding_model
        self.GROQ_API_KEY = groq_api_key or os.getenv("GROQ_API_KEY")
        self.MODEL_NAME = model_name
        self.GROQ_API_URL = groq_api_url

        self.chunks, self.metadata = self.load_course_chunks()
        self.index, self.model, self.embeddings = self.build_faiss_index(self.chunks)

    def load_course_chunks(self):
        # with open(self.JSON_FILE, "r", encoding="utf-8") as f:
        #     raw_courses = json.load(f)
        try:
            from pymongo import MongoClient
            # Fix: Use the properly formatted connection string
            print("Attempting to connect to MongoDB...")
            client = MongoClient("mongodb+srv://ankush_10010:hl9B_iFzL%5DBH%3BK2%3F@iititutor.rhtimwk.mongodb.net/?retryWrites=true&w=majority&appName=IITITutor", serverSelectionTimeoutMS=5000)

            # Test the connection
            client.admin.command('ping')
            print("MongoDB connection successful!")

            db = client["IITI_Tutor_DB"]
            collection = db["First_Year_Curriculum"]

            raw_courses = list(collection.find({}))
            print(f"Found {len(raw_courses)} courses in database")

            # Debug: Print first course structure to understand the data
            # if raw_courses:
            #     print("Sample course structure:")
            #     sample_course = raw_courses[0]
            #     print(f"Keys in first course: {list(sample_course.keys())}")
            #     for key, value in sample_course.items():
            #         print(f"  {key}: {type(value)} - {str(value)[:100]}...")

        except Exception as e:
            print(f"MongoDB connection failed: {e}")
            print("Using the original JSON-like processing method...")
            raw_courses = []
        finally:
            if 'client' in locals():
                client.close()

        chunks = []
        metadata = []

        for course in raw_courses:
            code = course.get("Course Code", "")
            title = course.get("Course Title", course.get("Title", ""))
            full_text = f"{code}\n{title}\n"

            for k, v in course.items():
                if isinstance(v, list):
                    full_text += f"\n{k}:\n" + "\n".join(
                        item if isinstance(item, str) else str(item) for item in v
                    )
                elif isinstance(v, dict):
                    full_text += f"\n{k}:\n" + json.dumps(v)
                elif isinstance(v, str):
                    full_text += f"\n{k}: {v}"

            for paragraph in full_text.split("\n\n"):
                if len(paragraph.strip()) > 50:
                    chunks.append(paragraph.strip())
                    metadata.append({"course": code, "title": title})

        return chunks, metadata

    def build_faiss_index(self, chunks):
        print_memory_usage("before build_faiss_index call")
        import faiss
        from sentence_transformers import SentenceTransformer
        import numpy as np
        model = SentenceTransformer(self.EMBEDDING_MODEL)
        embeddings = model.encode(chunks, show_progress_bar=True)
        index = faiss.IndexFlatL2(len(embeddings[0]))
        index.add(np.array(embeddings))
        print_memory_usage("after build_faiss_index call")
        return index, model, embeddings

    def extract_course_code(self, query: str, chat_history: list = None):
        print_memory_usage("before extract_course_code call")
        # Try to extract from current query first
        match = re.search(r"\b([A-Z]{2,3}\s?\d{3}[A-Z]?)\b", query.upper())
        if match:
            print_memory_usage("after extract_course_code call (from current query)")
            return match.group(1).replace(" ", "")

        # If not found in current query, check chat history
        if chat_history:
            # Iterate in reverse to get the most recent user message
            for msg in reversed(chat_history):
                if msg.get("role") == "user" and msg.get("content"):
                    history_match = re.search(r"\b([A-Z]{2,3}\s?\d{3}[A-Z]?)\b", msg["content"].upper())
                    if history_match:
                        print_memory_usage("after extract_course_code call (from chat history)")
                        return history_match.group(1).replace(" ", "")

        print_memory_usage("after extract_course_code call (no course code found)")
        return None


    def get_chunks_by_course_code(self, course_code):
        print_memory_usage("before get_chunks_by_course_code call")
        course_code_clean = course_code.upper().replace(" ", "")
        chunks_found = []
        for i, chunk in enumerate(self.chunks):
            code_in_chunk = self.metadata[i]["course"].upper().replace(" ", "")
            if course_code_clean in code_in_chunk:
                chunks_found.append((chunk, self.metadata[i]))
        print_memory_usage("after get_chunks_by_course_code call")
        return chunks_found

    def retrieve_relevant_chunks(self, query, index, model, chunks, metadata, chat_history: list = None, top_k=4):
        print_memory_usage("before retrieve_relevant_chunks call")
        course_code = self.extract_course_code(query, chat_history) # Pass chat_history here
        if course_code:
            chunks_for_course = self.get_chunks_by_course_code(course_code)
            if chunks_for_course:
                return chunks_for_course[:top_k]

        query_vec = self.model.encode([query])
        D, I = self.index.search(query_vec, top_k)
        print_memory_usage("after retrieve_relevant_chunks call")
        return [(self.chunks[i], self.metadata[i]) for i in I[0]]

    async def query_llama(self,query, context_chunks, chat_history: list = None):
      try:
        context = "\n\n".join(f"Chunk: {chunk}" for chunk, meta in context_chunks)
        messages = []
        messages.append({"role": "system", "content": "You are a helpful academic assistant. Use the following course data to answer questions."})

        if chat_history:
            for msg in chat_history:
                messages.append(msg)

        messages.append({"role": "user", "content": f"{context}\n\nQuestion: {query}"})

        payload = {
            "model": self.MODEL_NAME,
            "messages": messages,
            "temperature": 0.2,
        }

        headers = {
            "Authorization": f"Bearer {self.GROQ_API_KEY}",
            "Content-Type": "application/json"
        }

        async with httpx.AsyncClient() as client:
                  print_memory_usage("before API call")
                  response = await client.post(self.GROQ_API_URL, headers=headers, json=payload, timeout=20)
                  response.raise_for_status()
                  data = response.json()
                  print_memory_usage("after API call")
                  return {
                          "text" : data["choices"][0]["message"]["content"].strip(),
                          "pdf_file" : None
                        }
      except Exception as e:
            raise Exception(f"Query failed: {str(e)}")

class QuestionPaperBot:
    def __init__(self, groq_api_key=None):
        self.api_key = groq_api_key or os.getenv("gsk_fi9VBbpzMLwCD4rt4q")
        self.client = OpenAI(api_key=self.api_key, base_url="https://api.groq.com/openai/v1")

    async def pdf_to_images(self, pdf_path):
        print_memory_usage("before pdf_to_images call")
        from pdf2image import convert_from_path
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as pool:
            images = await loop.run_in_executor(pool, convert_from_path, pdf_path, 300)
            for i, page in enumerate(images):
                path = f"/content/Page_{i+1}.jpg"
                page.save(path, "JPEG")
                print(f"Saved: {path}")
            print_memory_usage("after pdf_to_images call")
            return len(images)

    async def extract_text(self, num):
        print_memory_usage("before extract_text call")
        import pytesseract
        from PIL import Image
        loop = asyncio.get_event_loop()
        async def process_image(i):
            image = Image.open(f"/content/Page_{i+1}.jpg")
            return pytesseract.image_to_string(image)
        tasks = [process_image(i) for i in range(num)]
        texts = await asyncio.gather(*tasks)
        text = "\n".join(texts)
        print("Extracted Text:\n", text)
        print_memory_usage("after extract_text call")
        return text

    async def generate_ans_paper_text(self, raw_text):
        print_memory_usage("before generate_ans_paper_text call")
        prompt = f"""You are an expert academic solution generator.

Your task is to:
1. Identify each distinct question from the following text.
2. Provide a detailed and accurate solution to each question.
3. Format your response such that each question is followed immediately by its solution.

For every question:
- Start with:
    Question <number>:
    <Full question text>

- Then give:
    Solution <number>
    <Answer formatted as per marking scheme below>

Answer formatting rules based on marks:
- If the question is of **1 mark**, give a **single-line concise explanation**.
- If the question is of **2 or 3 marks**, give the answer in **3 distinct and clear points** (use bulleted format only).
- If the question is of **4 or more marks**, structure your answer into **5 detailed and logical bullet points**.

Please ensure that:
- Sub-parts of a question (like (a), (b), etc.) are included within the same question block.
- Explanations are clear, precise, and maintain academic tone.
- Use correct mathematical notation and formatting where applicable. For mathematical expressions, use KaTeX syntax
  (e.g., $$...$$ for display math and $...$ for inline math).
- You do not skip any questions.

Now, here is the question paper text:

{raw_text}

Give your output in the same format as the input."""
        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.7,
            "max_tokens": 2048
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        try:
          async with httpx.AsyncClient() as client:
                    response = await client.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=payload, timeout=20)
                    print("Groq API Raw Response:", response.status_code, response.text)
                    response.raise_for_status()
                    data = response.json()
                    print_memory_usage("after generate_ans_paper_text call")
                    return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
              raise Exception(f"Query failed: {str(e)}")

        # print("Solutions:\n", result)
        # return result

    async def generate_question_paper_text(self, raw_text):
        print_memory_usage("before generate_question_paper_text call")
        prompt = f"""You are an expert question paper generator. Given the input question paper in a structured format, generate a new question paper that:
- Has the same structure
- Covers the same curriculum or topic
- Uses different wording and questions (same difficulty level)
- Keeps the same marks per question

Here is the input:

{raw_text}

Give your output in the same format as the input."""

        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.7,
            "max_tokens": 2048
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",

        }
        try:
          async with httpx.AsyncClient() as client:
                    response = await client.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=payload, timeout=20)
                    print("Groq API Raw Response:", response.status_code, response.text)
                    response.raise_for_status()
                    data = response.json()
                    print_memory_usage("after generate_question_paper_text call")
                    return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
              raise Exception(f"Query failed: {str(e)}")

        # print("Generated Paper:\n", result)
        # return result

    def text_to_formatted_pdf(self, text, filename="generated_pdf.pdf"):
      print_memory_usage("before text_to_formatted_pdf call")
      from reportlab.lib.pagesizes import A4
      from reportlab.pdfgen import canvas
      buffer = io.BytesIO()
      c = canvas.Canvas(buffer, pagesize=A4)
      width, height = A4

      x_margin = 50
      y = height - 50
      line_height = 14
      font_size = 11

      initial_lines = text.split('\n')
      #print(lines)
      lines= self.split_lines_to_fit_page(initial_lines)

      for i, line in enumerate(lines):
          clean_line = line.strip()

          # If it's within the first 5 lines, center-align the title
          if i < 5 and clean_line:
              c.setFont("Times-Bold", 14)
              text_width = c.stringWidth(clean_line, "Times-Bold", 14)
              c.drawString((width - text_width) / 2, y, clean_line)
              y -= line_height
          elif clean_line.lower().startswith("part") or clean_line.lower().startswith("section") or clean_line.lower().startswith("instructions") or clean_line.lower().startswith("questions") :
              c.setFont("Times-Bold", 12)  # Make section headings bold
              c.drawString(x_margin, y, clean_line)
              y -= line_height
          elif clean_line == "":
              y -= line_height // 2
          else:
              c.setFont("Times-Roman", font_size)
              c.drawString(x_margin, y, clean_line)
              y -= line_height

          if y < 50:
              c.showPage()
              y = height - 50
              c.setFont("Times-Roman", font_size)

      c.save()
      buffer.seek(0)  # Rewind to the start of the buffer
      print_memory_usage("after text_to_formatted_pdf call")
      return {
         "text" : lines,
         "pdf_file" : buffer
      }
    def wrap_text(self,text, canvas_obj, font_name, font_size, max_width):
      print_memory_usage("before wrap_text call")
      from reportlab.lib.pagesizes import A4
      from reportlab.pdfgen import canvas
      """
      Wrap a single string into multiple lines so it fits within max_width.
      Returns a list of wrapped lines.
      """
      words = text.split()
      lines = []
      current_line = ""

      for word in words:
          test_line = f"{current_line} {word}".strip()
          if canvas_obj.stringWidth(test_line, font_name, font_size) <= max_width:
              current_line = test_line
          else:
              lines.append(current_line)
              current_line = word
      if current_line:
          lines.append(current_line)
      print_memory_usage("after wrap_text call")
      return lines
    def split_lines_to_fit_page(self,lines, font_name="Times-Roman", font_size=11, page_size=A4, margin=50):
      print_memory_usage("before split_lines_to_fit_page call")
      from reportlab.pdfgen import canvas
      """
      Takes a list of strings (lines of text) and returns a new list
      where long lines are split so they fit on the PDF page.
      """
      dummy_canvas = canvas.Canvas("dummy.pdf")  # Only used to measure text width
      page_width, _ = page_size
      usable_width = page_width - 2 * margin

      fitted_lines = []
      for line in lines:
          if line.strip() == "":
              fitted_lines.append("")
          else:
              wrapped = self.wrap_text(line, dummy_canvas, font_name, font_size, usable_width)
              fitted_lines.extend(wrapped)
      print_memory_usage("after split_lines_to_fit_page call")
      return fitted_lines

    async def generate_question_paper(self,pdf_path):
        print_memory_usage("before generate_question_paper call")
        length=await self.pdf_to_images(pdf_path)
        print(f"PDF converted into {length} images.")
        Text=await self.extract_text(length)
        print("Extracted Text:\n", Text[:500])
        generated_text=await self.generate_question_paper_text(Text)
        print("Generated Answer Text:\n", generated_text[:500])
        print_memory_usage("after generate_question_paper call")
        return self.text_to_formatted_pdf(generated_text)

    async def generate_ans_paper(self,pdf_path):
        print_memory_usage("before generate_ans_paper call")
        length= await self.pdf_to_images(pdf_path)
        print(f"PDF converted into {length} images.")
        Text=await self.extract_text(length)
        print("Extracted Text:\n", Text[:500])
        generated_text= await self.generate_ans_paper_text(Text)
        print("Generated Answer Text:\n", generated_text[:500])
        print_memory_usage("after generate_ans_paper call")
        return self.text_to_formatted_pdf(generated_text)

# question_bot=QuestionPaperBot("gsk_XeRVOcvKlnGPiU2c5uqS")
# question_bot.generate_question_paper("/content/Eco_Paper.pdf")

# question_bot.generate_ans_paper("/content/question_paper.pdf")

#how to use
  # """we will take the prompt and pdf from the user , with the prompt we will decide what bot to choose and in this case , what functon to choose (to generate question or answer)"""
  # bot = QuestionPaperBot(groq_api_key="gsk_UZKlnGPiU2c5uqS")
  # bot.generate_question_paper(pdf_path="/content/Eco_Paper.pdf")


class Scheduler:
    def __init__(self, api_key: Optional[str] = None, model_name: str = "llama-3.1-8b-instant"):
        self.GROQ_API_KEY = api_key or os.getenv("GROQ_API_KEY")
        if not self.GROQ_API_KEY:
            raise ValueError("GROQ_API_KEY not found. Please set it as an environment variable or pass it to the Scheduler constructor.")

        self.MODEL_NAME = model_name
        self.GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"
        self.TIME_ZONE = "Asia/Kolkata" # IST

    async def _query_groq(self, prompt: str, max_tokens: int = 2048, temperature: float = 0.7) -> str:
        """Internal method to query the Groq API."""
        headers = {"Authorization": f"Bearer {self.GROQ_API_KEY}"}
        payload = {
            "model": self.MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(self.GROQ_API_URL, headers=headers, json=payload, timeout=30)
                response.raise_for_status()
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
            raise Exception(f"Query failed: {str(e)}")

    def build_schedule_prompt(self, task_description: str) -> str:
        """Builds a flexible prompt for schedule generation with explicit formatting."""
        return f"""
You are an expert productivity assistant. Your task is to create a structured and realistic schedule based on the user's request provided below.

**--- INSTRUCTIONS ---**

1.  **Analyze the Request:** Carefully read the user's request inside the `<user_request>` tags to understand:
    *   The total duration for the schedule (e.g., "3 days," "a week," "today").
    *   A complete list of all tasks to be scheduled.
    *   Specific constraints, such as wake-up/sleep times, task durations, daily recurring tasks, and appointments.
    *   If wake-up/sleep times are not stated, assume 8:00 AM wake-up and 10:30 PM sleep.

2.  **Create a Logical Schedule:**
    *   Distribute all tasks logically across the specified number of days.
    *   Include realistic time for meals (lunch around 1-2 PM, dinner around 8-9 PM) and short breaks.
    *   The schedule for each day MUST start at the wake-up time and end at the sleep time.
    *   Time slots must be contiguous.

3.  **Output Format (Follow EXACTLY):**
    *   The entire output must be only the schedule. Do not add any conversational text, summaries, or explanations.
    *   The format must match this example precisely:

üìÖ Your Personalized Schedule
============================================================
Note: This schedule assumes a wake-up time of 8:00 AM and a sleep time of 10:30 PM unless specified otherwise in the request.

üóì DAY 1
----------------------------------------
Time                 | Task
----------------------------------------
8:00 AM - 9:00 AM    | Morning Routine & Breakfast
9:00 AM - 12:00 PM   | Deep Work Block (e.g., Task 1)
12:00 PM - 1:00 PM   | Lunch Break
...
9:30 PM - 10:30 PM   | Wind down

üóì DAY 2
----------------------------------------
Time                 | Task
----------------------------------------
8:00 AM - 9:00 AM    | Morning Routine & Breakfast
... and so on for all required days.

**--- USER REQUEST ---**

<user_request>
{task_description}
</user_request>

Now, generate the schedule based on all the rules and the exact format described above.
"""

    def parse_and_format_schedule(self, raw_text: str) -> str:
        """
        Passes through the raw AI text, as the prompt now enforces strict formatting.
        A simple cleanup can be done if needed, but for now, we trust the prompt.
        """
        return raw_text.strip()


    async def run_scheduler(self, initial_prompt: str):
        """Main function to run the personal planner based on a single user prompt."""
        print("üß† AI Personal Planner")
        print("=" * 60)
        try:
            print(f"Received user prompt: {initial_prompt}")
            print("\n‚è≥ Generating your schedule...")

            # Build the single, powerful prompt
            llm_prompt = self.build_schedule_prompt(initial_prompt)

            # Query the LLM
            raw_llm_result = await self._query_groq(llm_prompt)

            print("\n‚úÖ Schedule generated by AI. Formatting it for you...")

            # Parse and format the output
            formatted_schedule = self.parse_and_format_schedule(raw_llm_result)

            result = {
                "text": formatted_schedule,
                "pdf_file": None
            }
            return result

        except Exception as e:
            print(f"Error during schedule generation: {e}")
            return {"error": f"Failed to generate schedule: {str(e)}"}

# scheduler_bot=Scheduler("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")
# scheduler_bot.main("i want to make a schedule for today , i have to go to gym for 2hours , study for 3 hours")


class RouterAgent:
    def __init__(self, groq_api_key=None, model="llama-3.1-8b-instant", chat_history_manager=None):
        self.api_key = groq_api_key or os.getenv("gsk_fi9VBbpLwCD4rt4q")
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        self.model = model
        self.chat_history_manager = chat_history_manager

    async def classify_prompt(self, user_prompt: str) -> str:
        """Use Groq LLM to classify the type of user query."""
        system_prompt = system_prompt = (
    "You are an intelligent routing assistant. Classify the user's prompt into exactly one of the following categories:\n\n"
    "1. questionpaper - Use this if the user wants to:\n"
    "   - Generate sample questions based on a question paper\n"
    "   - Get solutions to questions from a PDF\n"
    "   - Solve this\n"
    "   - Answer or solve questions from a document\n\n"
    "2. scheduler - Use this if the user wants to:\n"
    "   - Create a study plan\n"
    "   - Generate a weekly/daily schedule\n"
    "   - Organize time for exams, assignments, or productivity\n\n"
    "3. query - Use this if the user is asking academic queries such as:\n"
    "   - Course codes (e.g., EE101, CS202)\n"
    "   - Curriculum details or semester-wise subjects\n"
    "   - Recommended books or reading material\n"
    "   - Specific topics covered in a course\n"
    "   - Syllabus-related information\n\n"
    "Return only one of these exact values: questionpaper, scheduler, or query.\n"
    "Do not explain your reasoning. Just return the category label."
)


        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.2,
            "max_tokens": 10
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        # response = requests.post(self.api_url, headers=headers, json=payload,timeout=20)
        try:
          async with httpx.AsyncClient() as client:
              response = await client.post("https://api.groq.com/openai/v1/chat/completions",headers=headers, json=payload, timeout=20)
          response.raise_for_status()
          data =  response.json()
          classification = data["choices"][0]["message"]["content"].strip().lower()
          return classification
        except httpx.HTTPStatusError as e:
            raise Exception(f"API request failed: {str(e)}")
        except Exception as e:
            raise Exception(f"Error classifying prompt: {str(e)}")

    async def route(self, user_prompt: str, user_id: str, conversation_id: str, file=None):
        print("trying to call the classifier bot")
        query_type = await self.classify_prompt(user_prompt)
        print("classifier bot called")
        print("query-type", query_type)

        # Load chat history
        chat_history = await self.chat_history_manager.load_history(user_id, conversation_id)
        print(f"Loaded chat history for user {user_id}, conversation {conversation_id}: {chat_history}")
        # Save user message
        try:
            await self.chat_history_manager.save_message(user_id, conversation_id, "user", user_prompt)
            print("message saved in chat history")
        except Exception as e:
            print(f"Error saving message: {str(e)}")
        result = None
        if query_type == "questionpaper":
            print("question paper call")
            result = await self.call_questionpaper_bot(user_prompt, file=file)
        elif query_type == "scheduler":
            print("scheduler bot call")
            result = await self.call_scheduler_bot(user_prompt)
        elif query_type == "query":
            print("query bot call")
            result = await self.call_query_bot(user_prompt, chat_history=chat_history)
        else:
            result = {"text": "‚ùå Unable to classify the query. Please try rephrasing.", "pdf_file": None}

        # Save assistant message
        if result and result.get("text"):
            await self.chat_history_manager.save_message(user_id, conversation_id, "assistant", result["text"])

        return result

    async def classify_question_answer(self, prompt , file=None):
          GROQ_API_URL="https://api.groq.com/openai/v1/chat/completions"

          headers = {"Authorization": f"Bearer {self.api_key}"}
          classification_prompt = f"""
                  You are a classification agent that determines whether a user wants to generate answers to a question paper or to generate a similar question paper.
                  Here are the rules:

                  -If the prompt mentions solving, answering, or providing answers to questions, then it's "answer".

                  -If the prompt mentions creating, generating, or making a similar/new paper based on the input, then it's "generate".

                  -Be strict and choose only one: "answer" or "generate".

                  === Classification Rules ===
                  Choose "answer" if the prompt includes or implies any of the following:
                  - 'solve this paper'
                  - 'give answers to this'
                  - 'generate solution'
                  - 'provide solutions'
                  - 'attempt this paper'
                  - 'explain the answers'
                  - 'answer the following'
                  - 'solve question number 3 from the file'
                  - 'I need solutions for this question paper'
                  - 'solve this'

                  Choose "generate" if the prompt includes or implies any of the following:
                  - 'create a question paper like this'
                  - 'generate questions based on this'
                  - 'make a similar paper'
                  - 'create a new test'
                  - 'generate sample questions from this'
                  - 'I want a mock paper similar to this'
                  - 'make a paper using this file'
                  - 'give me a new set of questions based on this'

                  === Important ===
                  - Be strict.
                  - Choose only one: "answer" or "generate".
                  - Do not explain or add anything else. Just return the label.

          Prompt:
          {prompt.strip()}
          Answer:
          """
          payload = {
              "model": self.model,
              "messages": [
                  {"role": "user", "content": classification_prompt}
              ],
              "temperature": 0.2,
              "max_tokens": 10
          }

          # response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=payload, timeout=20)
          # response.raise_for_status()
          try:
            async with httpx.AsyncClient() as client:
                response = await client.post(self.api_url, headers=headers, json=payload, timeout=20)
                response.raise_for_status()
                data = response.json()
                result = data["choices"][0]["message"]["content"].strip().lower()
                return "answer" if "answer" in result else "question"
          except httpx.HTTPStatusError as e:
              raise Exception(f"API request failed: {str(e)}")
          except Exception as e:
              raise Exception(f"Error classifying question type: {str(e)}")

    async def call_questionpaper_bot(self,prompt,file=None):
      try:
            print("Classifying prompt")
            question_type = await self.classify_question_answer(prompt, file)
            print("Checkpoint1")
            questionP_bot = QuestionPaperBot(groq_api_key=self.api_key)
            print("Checkpoint2")
            if question_type == "answer":
                print("Solution Generator")
                result = await questionP_bot.generate_ans_paper(file)
                print("checkpoint 3")  # Ensure this is async
            else:
                print("Paper Generator")
                try:
                  result = await questionP_bot.generate_question_paper(file)
                  print("Checkpoint 3")  # Ensure this is async
                except Exception as e:
                  print("Error in question paper generation:", str(e))
                  raise
            return result
      except Exception as e:
            return {"error": f"Failed to process question paper: {str(e)}"}
    async def call_scheduler_bot(self, prompt):
        """scheduler bot wala part me do functions hai , ek weekly basis scheduler karne wali
        aur ek daily ki schedule karne wali , so we will take the user prompt and see ki wo kiss type  ka
        schedule banana chah rha 1day or 1 week and call the fucntuons on that basis"""
        try:
            scheduler_bot = Scheduler("gsk_fi9VBbpzLwCD4rt4q")
            result = await scheduler_bot.run_scheduler(prompt)
            return result
        except Exception as e:
            return {"error": f"Failed to generate schedule: {str(e)}"}

    async def call_query_bot(self, prompt: str, chat_history: list = None):
        try:
            print("query bot section")
            query_bot = QueryBot()  # Pass API key if needed
            chunks, metadata = query_bot.load_course_chunks()  # Assume sync for now
            index, model, _ = query_bot.build_faiss_index(chunks)  # Assume sync
            relevant_chunks = query_bot.retrieve_relevant_chunks(prompt, index, model, chunks, metadata, chat_history=chat_history)  # Pass chat_history here
            print("ü§ñ Thinking...")
            answer = await query_bot.query_llama(prompt, relevant_chunks, chat_history=chat_history)  # Make this async if it involves I/O
            return answer
        except Exception as e:
            return {"error": f"Failed to process query: {str(e)}"}


app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8080"],  # Or set specific origins like ["http://localhost:3000"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


mongo_client = motor.motor_asyncio.AsyncIOMotorClient("mongodb+srv://ankush_10010:hl9B_iFzL%5DBH%3BK2%3F@iititutor.rhtimwk.mongodb.net/?retryWrites=true&w=majority&appName=IITITutor")
chat_history_manager = ChatHistoryManager(mongo_client)

router_agent = RouterAgent("gsk_7ceNFpxDy3AauyQJWKk", chat_history_manager=chat_history_manager)
#groq api wagera ham code ke andar hi fix kardenge (init ke time i mean)
@app.post("/route")
async def route_handler(
    request: Request,
    response: Response,
    prompt: str = Form(...),
    file: Optional[UploadFile] = File(None)
):
    temp_file_path = None
    print("under the async function")
    # Handle file upload if provided
    if file:
        # Save file to a temp location so pdf2image can use it
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp:
            shutil.copyfileobj(file.file, temp)
            temp_file_path = temp.name
        print("Saved uploaded file to:", temp_file_path)

    # Determine user_id and conversation_id
    current_user_id = request.cookies.get("user_id")
    current_conversation_id = request.cookies.get("convo_id")

    print(f"Received user_id from cookie: {current_user_id}")
    print(f"Received convo_id from cookie: {current_conversation_id}")

    if not current_user_id or current_user_id == "None":
        current_user_id = str(uuid.uuid4())
        print(f"Generated new user_id: {current_user_id}")
    else:
        print(f"Using existing user_id: {current_user_id}")

    if not current_conversation_id or current_conversation_id == "None":
        current_conversation_id = str(uuid.uuid4())
        print(f"Generated new convo_id: {current_conversation_id}")
    else:
        print(f"Using existing convo_id: {current_conversation_id}")

    try:
        print("trying to call the router agent")
        result = await router_agent.route(prompt, current_user_id ,current_conversation_id, temp_file_path)
        print("router agent called")
        print_memory_usage("after router call")
        print("Routing function is done")
        print("Result dict:", result)
    except Exception as e:
        return JSONResponse(content={"error": str(e)})

    print("checkpoint 4")
    # Clean up the file after use
    if temp_file_path and os.path.exists(temp_file_path):
        os.remove(temp_file_path)

    print("checkpoint 5")
    # result must be a dict like {"text": ..., "pdf_file": ...}
    text = result.get("text", "")
    pdf_file = result.get("pdf_file", None)
    if not text:
      print("Text is empty or None")

    print("text value:", text, "Type:", type(text), "Truthy?", bool(text))
    print("pdf_file value:", pdf_file, "Type:", type(pdf_file), "Truthy?", bool(pdf_file))

    if hasattr(pdf_file, "getbuffer"):
        print("pdf size:", len(pdf_file.getbuffer()))

    # Only PDF
    if pdf_file:
      pdf_file.seek(0)
      print("pdf file is there")
      print(pdf_file)
    if text and not pdf_file:
      print("text only")
      try:
          if isinstance(text, list):
              text_str = "\n".join(text)
          else:
              text_str = str(text)
          # Set cookie parameters based on the environment
          is_secure = True
          # 'lax' is a good default for dev; 'none' is needed for prod cross-site scenarios
          # same_site_policy = "none" if is_secure else "lax"
          resp= JSONResponse(
              content={"text": text_str},
              media_type="application/json"
          )
          resp.set_cookie(
              key="user_id",
              value=current_user_id,
              max_age=86400,
              samesite="None",
              secure=True,
              path="/"
          )
          resp.set_cookie(
              key="convo_id",
              value=current_conversation_id,
              max_age=86400,
              samesite="None",
              secure=True,
              path="/"
          )
          return resp

      except Exception as e:
          print("Text-only Response Error:", e)
          resp= JSONResponse(
              content={"error": "Failed to return text-only response."},
              media_type="application/json",
              status_code=500
          )
          resp.set_cookie(
              key="user_id",
              value=current_user_id,
              max_age=86400,
              samesite="None",
              secure=True,
              path="/"
          )
          resp.set_cookie(
              key="convo_id",
              value=current_conversation_id,
              max_age=86400,
              samesite="None",
              secure=True,
              path="/"
          )
          return resp

    if pdf_file and not text:
      try:
        print("only pdf")
        resp= StreamingResponse(
            pdf_file,
            media_type="application/pdf",
            headers={"Content-Disposition": "attachment; filename=question_paper.pdf"}
        )
        resp.set_cookie(
            key="user_id",
            value=current_user_id,
            max_age=86400,
            samesite="None",
            secure=True,
            path="/"
        )
        resp.set_cookie(
            key="convo_id",
            value=current_conversation_id,
            max_age=86400,
            samesite="None",
            secure=True,
            path="/"
        )
        return resp
      except Exception as e:
        print("PDF Streaming Error:", e)
        return JSONResponse({"error": "Failed to stream PDF."})
    if text and pdf_file:
        print("pdf+text")
        if isinstance(text, list):
            text_str = "\n".join(text)
        else:
            text_str = str(text)

        # Convert bytes to file-like object for streaming

        pdf_file.seek(0)
        pdf_bytes = pdf_file.read()
        print("PDF byte size:", len(pdf_bytes))

        # Create multipart response
        boundary = "----Boundary"
        parts = []

        # Part 1: JSON text
        json_part = (
            f'--{boundary}\r\n'
            f'Content-Type: application/json\r\n\r\n'
            f'{json.dumps({"text": text_str})}\r\n'
        )
        parts.append(json_part.encode('utf-8'))

        # Part 2: PDF
        pdf_part = (
                    f'--{boundary}\r\n'
                    f'Content-Type: application/pdf\r\n'
                    f'Content-Disposition: attachment; filename=question_paper.pdf\r\n\r\n'
                ).encode('utf-8') + pdf_bytes

        parts.append(pdf_part)

        # End boundary
        parts.append(f'--{boundary}--\r\n'.encode('utf-8'))

        # Combine parts
        body = b''.join(parts)

        try:
            resp= Response(
                content=body,
                media_type=f'multipart/mixed; boundary={boundary}',
                headers={"Content-Length": str(len(body))}
            )
            resp.set_cookie(
                key="user_id",
                value=current_user_id,
                max_age=86400,
                samesite="None",
                secure=True,
                path="/"
            )
            resp.set_cookie(
                key="convo_id",
                value=current_conversation_id,
                max_age=86400,
                samesite="None",
                secure=True,
                path="/"
            )
            return resp
        except Exception as e:
            print("Multipart Response Error:", e)
            return Response(
                content=json.dumps({"error": "Failed to stream PDF and text."}),
                media_type="application/json",
                status_code=500
            )

    # Fallback
    resp= Response(
        content=json.dumps({
            "text": prompt if prompt else "",
            "pdf_download_url": "/generated/question_paper.pdf"
        }),
        media_type="application/json"
    )
    resp.set_cookie(
    key="user_id",
    value=current_user_id,
    max_age=86400,
    samesite="None",
    secure=True,
    path="/"
)
    resp.set_cookie(
        key="convo_id",
        value=current_conversation_id,
        max_age=86400,
        samesite="None",
        secure=True,
        path="/"
    )
    return resp



import uvicorn
from pyngrok import ngrok
import nest_asyncio
!ngrok authtoken 2sXyT7gzUwfdxxSHL1
# Allow async in Colab
nest_asyncio.apply()
# Start ngrok tunnel
public_url = ngrok.connect(8000)
print("Public URL:", public_url)

# Run FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)
